import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Suponiendo que tienes un DataFrame 'df' con tus datos
# Aquí generamos un DataFrame de ejemplo
np.random.seed(0)
df = pd.DataFrame(np.random.rand(100, 70), columns=[f'Var{i+1}' for i in range(70)])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Crear el modelo PCA y ajustar los datos
pca = PCA(n_components=2)  # Especificar el número de componentes principales que deseas
principalComponents = pca.fit_transform(scaled_data)

# Crear un DataFrame con los componentes principales
principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print('Varianza explicada por cada componente principal:')
print(explained_variance)

# Obtener las cargas de los componentes principales
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

# Crear un DataFrame con las cargas
loadings_df = pd.DataFrame(loadings, columns=[f'PC{i+1}' for i in range(pca.n_components_)], index=df.columns)

# Mostrar las primeras filas del DataFrame de cargas
print(loadings_df.head())

# Si deseas ver las variables más influyentes para cada componente principal
n_top_variables = 5  # Número de variables más influyentes que deseas ver
for i in range(pca.n_components_):
    sorted_loadings = loadings_df.iloc[:, i].abs().sort_values(ascending=False)
    print(f'Component {i+1}:')
    print(sorted_loadings.head(n_top_variables))







##############################################################################################################

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Generar un DataFrame de ejemplo con 70 variables y 100 observaciones
np.random.seed(0)
df = pd.DataFrame(np.random.rand(100, 70), columns=[f'Var{i+1}' for i in range(70)])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Crear el modelo PCA y ajustar los datos
pca = PCA()
principalComponents = pca.fit_transform(scaled_data)

# Varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance)

# Mostrar la varianza explicada por cada componente principal
print("Varianza explicada por cada componente principal:")
for i, var in enumerate(explained_variance):
    print(f"PC{i+1}: {var:.4f}")

# Mostrar la varianza explicada acumulada
print("\nVarianza explicada acumulada:")
for i, var in enumerate(cumulative_explained_variance):
    print(f"PC{i+1}: {var:.4f}")

# Obtener las cargas de los componentes principales
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
loadings_df = pd.DataFrame(loadings, columns=[f'PC{i+1}' for i in range(pca.n_components_)], index=df.columns)

# Mostrar las primeras filas del DataFrame de cargas
print("\nCargas de los componentes principales:")
print(loadings_df.head())

# Crear un DataFrame con los scores de los componentes principales
scores_df = pd.DataFrame(principalComponents, columns=[f'PC{i+1}' for i in range(pca.n_components_)])

# Graficar la varianza explicada acumulada
plt.figure(figsize=(8,6))
plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Explicada Acumulada')
plt.title('Codo de Scree Plot')
plt.axhline(y=0.90, color='r', linestyle='--', label='90% Varianza Explicada')
plt.legend(loc='best')
plt.grid()
plt.show()

# Graficar los datos en el espacio de los primeros dos componentes principales
plt.figure(figsize=(8,6))
plt.scatter(scores_df['PC1'], scores_df['PC2'])
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('Gráfico de Dispersión de los Primeros Dos Componentes Principales')
plt.show()




######################################################################################################



Sí, tienes razón en que el PCA es útil para visualizar el comportamiento conjunto de un gran número de variables en un espacio de menor dimensión, generalmente en 2D (\( \mathbb{R}^2 \)) o 3D (\( \mathbb{R}^3 \)). Aquí te doy más información y correcciones para aclarar cómo y por qué se utiliza PCA:

### Conceptos Básicos del PCA

1. **Reducción de Dimensionalidad**:
   - **Propósito**: El principal propósito del PCA es reducir la dimensionalidad de un conjunto de datos con muchas variables (en tu caso, 70) a un número menor de componentes principales, manteniendo la mayor parte de la varianza (información) posible.
   - **Espacio de Menor Dimensión**: Al reducir la dimensionalidad, puedes visualizar datos en un espacio de menor dimensión (\( \mathbb{R}^2 \) para dos componentes, \( \mathbb{R}^3 \) para tres componentes, etc.), lo que facilita la interpretación y la visualización.

2. **Componentes Principales**:
   - **Definición**: Los componentes principales son nuevas variables que se construyen como combinaciones lineales de las variables originales. Estas nuevas variables son ortogonales entre sí y están ordenadas de manera que el primer componente principal captura la mayor parte de la varianza de los datos, el segundo componente principal captura la mayor parte de la varianza restante, y así sucesivamente.
   - **Varianza Explicada**: La varianza explicada por cada componente principal indica cuánta información (varianza) de los datos originales se retiene en ese componente.

### Visualización con PCA

Cuando reduces tus 70 variables a 2 componentes principales y los visualizas en \( \mathbb{R}^2 \), estás proyectando los datos originales en un plano donde se puede ver cómo se comportan las principales tendencias y relaciones en los datos. Esta visualización puede ayudarte a:

1. **Identificar Patrones**: Ver agrupaciones, tendencias y relaciones entre las observaciones.
2. **Detectar Outliers**: Identificar puntos de datos que se desvían significativamente de los demás.
3. **Simplificar la Interpretación**: Facilitar la interpretación al reducir la complejidad de los datos.

### Proceso del PCA

1. **Estandarización**: Se estandarizan las variables originales para que todas tengan media cero y varianza uno. Esto asegura que todas las variables contribuyan equitativamente al análisis.
2. **Cálculo de la Matriz de Covarianza**: Se calcula la matriz de covarianza de los datos estandarizados para entender cómo varían las variables conjuntamente.
3. **Descomposición en Valores Propios y Vectores Propios**: Se descompone la matriz de covarianza en sus valores propios (eigenvalues) y vectores propios (eigenvectors). Los vectores propios se convierten en los componentes principales.
4. **Proyección de los Datos**: Los datos originales se proyectan en los nuevos ejes definidos por los componentes principales.

### Ejemplo en Python

Aquí hay un ejemplo de cómo aplicar PCA y visualizar los datos en \( \mathbb{R}^2 \):

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Generar un DataFrame de ejemplo con 70 variables y 100 observaciones
np.random.seed(0)
df = pd.DataFrame(np.random.rand(100, 70), columns=[f'Var{i+1}' for i in range(70)])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Crear el modelo PCA y ajustar los datos
pca = PCA(n_components=2)  # Reducir a 2 componentes principales para visualización
principalComponents = pca.fit_transform(scaled_data)

# Crear un DataFrame con los componentes principales
principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])

# Graficar los datos en el espacio de los primeros dos componentes principales
plt.figure(figsize=(8,6))
plt.scatter(principalDf['PC1'], principalDf['PC2'])
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('Gráfico de Dispersión de los Primeros Dos Componentes Principales')
plt.show()
```

### Conclusión

El PCA te permite reducir la complejidad de un conjunto de datos con muchas variables, 
proyectándolos en un espacio de menor dimensión (\( \mathbb{R}^2 \) para dos componentes principales). 
Esta proyección facilita la visualización e interpretación de los datos, ayudándote a identificar patrones, 
agrupaciones y outliers de manera más clara. Por lo tanto, el PCA es una herramienta valiosa en el análisis exploratorio de datos 
y en la preparación de datos para modelado predictivo.


